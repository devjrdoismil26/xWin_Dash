"""
üé§ SPEECH PROCESSOR - Whisper para Business Intelligence

Capacidades:
- Transcri√ß√£o de √°udio em m√∫ltiplos idiomas
- An√°lise de sentimento de fala
- Extra√ß√£o de insights de reuni√µes
- An√°lise de chamadas de vendas
- Processamento de feedback por voz
- Detec√ß√£o de emo√ß√µes na fala
- Resumo autom√°tico de conversas
"""

import whisper
import torch
import librosa
import numpy as np
import asyncio
import logging
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from enum import Enum
import json
import base64
import io
import tempfile
import os
from datetime import datetime, timedelta
import wave
import webrtcvad
from pydub import AudioSegment
from pydub.silence import split_on_silence
import openai

# Configure logging
logger = logging.getLogger(__name__)

class SpeechAnalysisType(Enum):
    TRANSCRIPTION = "transcription"
    MEETING_ANALYSIS = "meeting_analysis"
    SALES_CALL_ANALYSIS = "sales_call_analysis"
    CUSTOMER_FEEDBACK = "customer_feedback"
    SENTIMENT_ANALYSIS = "sentiment_analysis"
    EMOTION_DETECTION = "emotion_detection"
    SPEAKER_IDENTIFICATION = "speaker_identification"
    CONVERSATION_SUMMARY = "conversation_summary"

class AudioFormat(Enum):
    WAV = "wav"
    MP3 = "mp3"
    M4A = "m4a"
    FLAC = "flac"
    OGG = "ogg"

@dataclass
class SpeechAnalysisRequest:
    audio_data: str  # Base64 encoded audio
    analysis_type: SpeechAnalysisType
    language: Optional[str] = None  # Auto-detect se None
    context: Optional[Dict[str, Any]] = None
    business_domain: Optional[str] = None
    speaker_names: Optional[List[str]] = None

@dataclass
class TranscriptionSegment:
    start_time: float
    end_time: float
    text: str
    confidence: float
    speaker: Optional[str] = None
    language: Optional[str] = None

@dataclass
class SpeechAnalysisResult:
    analysis_type: SpeechAnalysisType
    transcription: List[TranscriptionSegment]
    insights: Dict[str, Any]
    summary: str
    confidence_score: float
    recommendations: List[str]
    metadata: Dict[str, Any]
    processing_time: float

class SpeechProcessor:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"Inicializando SpeechProcessor no device: {self.device}")
        
        # Carregar modelos
        self._load_models()
        
        # Configurar VAD (Voice Activity Detection)
        self.vad = webrtcvad.Vad(2)  # Agressividade m√©dia
        
        # Cliente OpenAI para an√°lises avan√ßadas
        self.openai_client = openai.AsyncOpenAI()

    def _load_models(self):
        """Carrega os modelos necess√°rios"""
        try:
            # Whisper para transcri√ß√£o
            self.whisper_model = whisper.load_model("large-v3", device=self.device)
            logger.info("‚úÖ Whisper large-v3 carregado")
            
            # Modelo menor para an√°lises r√°pidas
            self.whisper_base = whisper.load_model("base", device=self.device)
            logger.info("‚úÖ Whisper base carregado")
            
        except Exception as e:
            logger.error(f"Erro ao carregar modelos: {e}")
            raise

    async def analyze(self, request: SpeechAnalysisRequest) -> SpeechAnalysisResult:
        """Analisa √°udio usando Whisper e outros modelos"""
        start_time = asyncio.get_event_loop().time()
        
        try:
            # Carregar e preprocessar √°udio
            audio_path = await self._load_audio(request.audio_data)
            
            # Transcri√ß√£o base
            transcription = await self._transcribe_audio(
                audio_path, 
                request.language,
                request.analysis_type
            )
            
            # An√°lises espec√≠ficas
            insights = await self._perform_specialized_analysis(
                transcription, 
                request.analysis_type,
                request.context,
                request.business_domain
            )
            
            # Gerar recomenda√ß√µes
            recommendations = await self._generate_recommendations(
                insights, 
                request.analysis_type,
                request.business_domain
            )
            
            # Limpeza
            os.unlink(audio_path)
            
            processing_time = asyncio.get_event_loop().time() - start_time
            
            return SpeechAnalysisResult(
                analysis_type=request.analysis_type,
                transcription=transcription,
                insights=insights,
                summary=await self._generate_summary(insights, request.analysis_type),
                confidence_score=np.mean([seg.confidence for seg in transcription]),
                recommendations=recommendations,
                metadata={
                    "models_used": ["Whisper", "GPT-4"],
                    "device": self.device,
                    "language_detected": transcription[0].language if transcription else None,
                    "duration": insights.get("audio_duration", 0)
                },
                processing_time=processing_time
            )
            
        except Exception as e:
            logger.error(f"Erro na an√°lise de √°udio: {e}")
            raise

    async def _load_audio(self, audio_data: str) -> str:
        """Carrega √°udio de base64 e salva temporariamente"""
        try:
            # Decodificar base64
            audio_bytes = base64.b64decode(audio_data)
            
            # Criar arquivo tempor√°rio
            with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as tmp_file:
                tmp_file.write(audio_bytes)
                return tmp_file.name
                
        except Exception as e:
            logger.error(f"Erro ao carregar √°udio: {e}")
            raise

    async def _transcribe_audio(
        self, 
        audio_path: str, 
        language: Optional[str] = None,
        analysis_type: SpeechAnalysisType = SpeechAnalysisType.TRANSCRIPTION
    ) -> List[TranscriptionSegment]:
        """Transcreve √°udio usando Whisper"""
        try:
            # Escolher modelo baseado no tipo de an√°lise
            model = self.whisper_model if analysis_type in [
                SpeechAnalysisType.MEETING_ANALYSIS,
                SpeechAnalysisType.SALES_CALL_ANALYSIS,
                SpeechAnalysisType.CONVERSATION_SUMMARY
            ] else self.whisper_base
            
            # Transcrever com timestamps
            result = model.transcribe(
                audio_path,
                language=language,
                task="transcribe",
                word_timestamps=True,
                condition_on_previous_text=False
            )
            
            # Converter para segmentos
            segments = []
            for segment in result["segments"]:
                segments.append(TranscriptionSegment(
                    start_time=segment["start"],
                    end_time=segment["end"],
                    text=segment["text"].strip(),
                    confidence=segment.get("avg_logprob", 0.0),
                    language=result.get("language")
                ))
            
            return segments
            
        except Exception as e:
            logger.error(f"Erro na transcri√ß√£o: {e}")
            return []

    async def _perform_specialized_analysis(
        self,
        transcription: List[TranscriptionSegment],
        analysis_type: SpeechAnalysisType,
        context: Optional[Dict[str, Any]],
        business_domain: Optional[str]
    ) -> Dict[str, Any]:
        """Realiza an√°lise especializada baseada no tipo"""
        
        # Texto completo da transcri√ß√£o
        full_text = " ".join([seg.text for seg in transcription])
        
        # Dura√ß√£o total
        total_duration = transcription[-1].end_time if transcription else 0
        
        base_insights = {
            "audio_duration": total_duration,
            "word_count": len(full_text.split()),
            "speaking_rate": len(full_text.split()) / (total_duration / 60) if total_duration > 0 else 0,
            "segment_count": len(transcription)
        }
        
        if analysis_type == SpeechAnalysisType.MEETING_ANALYSIS:
            specialized = await self._analyze_meeting(full_text, transcription, context)
        elif analysis_type == SpeechAnalysisType.SALES_CALL_ANALYSIS:
            specialized = await self._analyze_sales_call(full_text, transcription, context)
        elif analysis_type == SpeechAnalysisType.CUSTOMER_FEEDBACK:
            specialized = await self._analyze_customer_feedback(full_text, context)
        elif analysis_type == SpeechAnalysisType.SENTIMENT_ANALYSIS:
            specialized = await self._analyze_sentiment(full_text)
        elif analysis_type == SpeechAnalysisType.EMOTION_DETECTION:
            specialized = await self._detect_emotions(full_text, transcription)
        elif analysis_type == SpeechAnalysisType.CONVERSATION_SUMMARY:
            specialized = await self._summarize_conversation(full_text, context)
        else:
            specialized = {}
        
        return {**base_insights, **specialized}

    async def _analyze_meeting(
        self, 
        full_text: str, 
        transcription: List[TranscriptionSegment],
        context: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """An√°lise espec√≠fica de reuni√µes"""
        try:
            prompt = f"""
            Analise esta transcri√ß√£o de reuni√£o e extraia insights de Business Intelligence:

            Transcri√ß√£o: {full_text}

            Retorne um JSON com:
            {{
                "meeting_type": "brainstorm|status|decis√£o|planejamento|outro",
                "key_decisions": ["decis√£o1", "decis√£o2"],
                "action_items": [
                    {{"task": "tarefa", "assignee": "respons√°vel", "deadline": "prazo"}}
                ],
                "main_topics": ["t√≥pico1", "t√≥pico2"],
                "participants_engagement": "alta|m√©dia|baixa",
                "meeting_effectiveness": "alta|m√©dia|baixa",
                "next_steps": ["pr√≥ximo passo 1", "pr√≥ximo passo 2"],
                "risks_identified": ["risco1", "risco2"],
                "opportunities": ["oportunidade1", "oportunidade2"],
                "sentiment_overall": "positivo|neutro|negativo"
            }}
            """
            
            response = await self.openai_client.chat.completions.create(
                model="gpt-4-turbo-preview",
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0.3
            )
            
            return json.loads(response.choices[0].message.content)
            
        except Exception as e:
            logger.error(f"Erro na an√°lise de reuni√£o: {e}")
            return {"error": str(e)}

    async def _analyze_sales_call(
        self, 
        full_text: str, 
        transcription: List[TranscriptionSegment],
        context: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """An√°lise espec√≠fica de chamadas de vendas"""
        try:
            prompt = f"""
            Analise esta transcri√ß√£o de chamada de vendas:

            Transcri√ß√£o: {full_text}

            Retorne um JSON com:
            {{
                "call_stage": "prospec√ß√£o|apresenta√ß√£o|obje√ß√£o|fechamento|follow_up",
                "customer_interest_level": "alto|m√©dio|baixo",
                "objections_raised": ["obje√ß√£o1", "obje√ß√£o2"],
                "pain_points_identified": ["dor1", "dor2"],
                "buying_signals": ["sinal1", "sinal2"],
                "next_action_recommended": "string",
                "deal_probability": 0-100,
                "key_quotes": ["frase importante 1", "frase importante 2"],
                "competitor_mentions": ["concorrente1", "concorrente2"],
                "budget_discussed": true/false,
                "decision_maker_present": true/false,
                "timeline_mentioned": "string ou null"
            }}
            """
            
            response = await self.openai_client.chat.completions.create(
                model="gpt-4-turbo-preview",
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0.3
            )
            
            return json.loads(response.choices[0].message.content)
            
        except Exception as e:
            logger.error(f"Erro na an√°lise de vendas: {e}")
            return {"error": str(e)}

    async def _analyze_customer_feedback(
        self, 
        full_text: str,
        context: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """An√°lise de feedback de clientes"""
        try:
            prompt = f"""
            Analise este feedback de cliente:

            Feedback: {full_text}

            Retorne um JSON com:
            {{
                "satisfaction_score": 1-10,
                "sentiment": "muito_positivo|positivo|neutro|negativo|muito_negativo",
                "main_issues": ["issue1", "issue2"],
                "praised_aspects": ["aspecto1", "aspecto2"],
                "improvement_suggestions": ["sugest√£o1", "sugest√£o2"],
                "urgency_level": "alta|m√©dia|baixa",
                "category": "produto|servi√ßo|suporte|pre√ßo|entrega|outro",
                "actionable_insights": ["insight1", "insight2"],
                "churn_risk": "alto|m√©dio|baixo",
                "upsell_opportunity": true/false
            }}
            """
            
            response = await self.openai_client.chat.completions.create(
                model="gpt-4-turbo-preview",
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0.3
            )
            
            return json.loads(response.choices[0].message.content)
            
        except Exception as e:
            logger.error(f"Erro na an√°lise de feedback: {e}")
            return {"error": str(e)}

    async def _analyze_sentiment(self, full_text: str) -> Dict[str, Any]:
        """An√°lise de sentimento detalhada"""
        try:
            prompt = f"""
            Fa√ßa uma an√°lise detalhada de sentimento:

            Texto: {full_text}

            Retorne um JSON com:
            {{
                "overall_sentiment": "muito_positivo|positivo|neutro|negativo|muito_negativo",
                "sentiment_score": -1.0 to 1.0,
                "emotions_detected": ["alegria", "raiva", "medo", "tristeza", "surpresa"],
                "emotion_intensities": {{"alegria": 0.0-1.0, "raiva": 0.0-1.0}},
                "key_phrases": ["frase positiva", "frase negativa"],
                "sentiment_progression": ["in√≠cio", "meio", "fim"],
                "confidence": 0.0-1.0
            }}
            """
            
            response = await self.openai_client.chat.completions.create(
                model="gpt-4-turbo-preview",
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0.2
            )
            
            return json.loads(response.choices[0].message.content)
            
        except Exception as e:
            logger.error(f"Erro na an√°lise de sentimento: {e}")
            return {"error": str(e)}

    async def _detect_emotions(
        self, 
        full_text: str, 
        transcription: List[TranscriptionSegment]
    ) -> Dict[str, Any]:
        """Detec√ß√£o de emo√ß√µes na fala"""
        try:
            # An√°lise por segmentos temporais
            emotion_timeline = []
            
            for segment in transcription[:10]:  # Primeiros 10 segmentos
                prompt = f"""
                Analise as emo√ß√µes neste trecho de fala:
                
                Texto: "{segment.text}"
                Tempo: {segment.start_time:.1f}s - {segment.end_time:.1f}s
                
                Retorne JSON:
                {{
                    "primary_emotion": "alegria|raiva|medo|tristeza|surpresa|neutro",
                    "intensity": 0.0-1.0,
                    "secondary_emotions": ["emo√ß√£o1", "emo√ß√£o2"]
                }}
                """
                
                response = await self.openai_client.chat.completions.create(
                    model="gpt-4-turbo-preview",
                    messages=[{"role": "user", "content": prompt}],
                    response_format={"type": "json_object"},
                    temperature=0.2
                )
                
                emotion_data = json.loads(response.choices[0].message.content)
                emotion_timeline.append({
                    "start_time": segment.start_time,
                    "end_time": segment.end_time,
                    **emotion_data
                })
            
            return {
                "emotion_timeline": emotion_timeline,
                "dominant_emotion": max(emotion_timeline, key=lambda x: x["intensity"])["primary_emotion"] if emotion_timeline else "neutro"
            }
            
        except Exception as e:
            logger.error(f"Erro na detec√ß√£o de emo√ß√µes: {e}")
            return {"error": str(e)}

    async def _summarize_conversation(
        self, 
        full_text: str,
        context: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Resumo inteligente de conversas"""
        try:
            prompt = f"""
            Crie um resumo executivo desta conversa:

            Conversa: {full_text}

            Retorne um JSON com:
            {{
                "executive_summary": "Resumo de 2-3 frases",
                "key_points": ["ponto1", "ponto2", "ponto3"],
                "decisions_made": ["decis√£o1", "decis√£o2"],
                "follow_up_actions": ["a√ß√£o1", "a√ß√£o2"],
                "participants_mentioned": ["pessoa1", "pessoa2"],
                "topics_covered": ["t√≥pico1", "t√≥pico2"],
                "conversation_outcome": "bem-sucedida|inconclusiva|conflituosa",
                "next_meeting_needed": true/false,
                "priority_level": "alta|m√©dia|baixa"
            }}
            """
            
            response = await self.openai_client.chat.completions.create(
                model="gpt-4-turbo-preview",
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0.3
            )
            
            return json.loads(response.choices[0].message.content)
            
        except Exception as e:
            logger.error(f"Erro no resumo de conversa: {e}")
            return {"error": str(e)}

    async def _generate_recommendations(
        self,
        insights: Dict[str, Any],
        analysis_type: SpeechAnalysisType,
        business_domain: Optional[str]
    ) -> List[str]:
        """Gera recomenda√ß√µes baseadas nos insights"""
        recommendations = []
        
        # Recomenda√ß√µes baseadas na taxa de fala
        speaking_rate = insights.get("speaking_rate", 0)
        if speaking_rate > 180:
            recommendations.append("Reduzir velocidade de fala para melhor compreens√£o")
        elif speaking_rate < 120:
            recommendations.append("Aumentar dinamismo na apresenta√ß√£o")
        
        # Recomenda√ß√µes espec√≠ficas por tipo
        if analysis_type == SpeechAnalysisType.SALES_CALL_ANALYSIS:
            deal_prob = insights.get("deal_probability", 0)
            if deal_prob < 30:
                recommendations.append("Focar em identificar e resolver obje√ß√µes")
            elif deal_prob > 70:
                recommendations.append("Acelerar processo de fechamento")
        
        elif analysis_type == SpeechAnalysisType.MEETING_ANALYSIS:
            effectiveness = insights.get("meeting_effectiveness", "m√©dia")
            if effectiveness == "baixa":
                recommendations.append("Melhorar estrutura e agenda das reuni√µes")
        
        return recommendations

    async def _generate_summary(
        self,
        insights: Dict[str, Any],
        analysis_type: SpeechAnalysisType
    ) -> str:
        """Gera resumo da an√°lise"""
        duration = insights.get("audio_duration", 0)
        word_count = insights.get("word_count", 0)
        
        return f"An√°lise {analysis_type.value}: {duration:.1f}s de √°udio, {word_count} palavras processadas."

    async def batch_transcribe(self, audio_files: List[str]) -> List[SpeechAnalysisResult]:
        """Transcri√ß√£o em lote"""
        tasks = []
        for audio_data in audio_files:
            request = SpeechAnalysisRequest(
                audio_data=audio_data,
                analysis_type=SpeechAnalysisType.TRANSCRIPTION
            )
            tasks.append(self.analyze(request))
        
        return await asyncio.gather(*tasks)

    def extract_audio_features(self, audio_path: str) -> Dict[str, Any]:
        """Extrai caracter√≠sticas t√©cnicas do √°udio"""
        try:
            # Carregar √°udio
            y, sr = librosa.load(audio_path)
            
            # Extrair features
            features = {
                "duration": float(librosa.get_duration(y=y, sr=sr)),
                "sample_rate": int(sr),
                "tempo": float(librosa.beat.tempo(y=y, sr=sr)[0]),
                "spectral_centroid": float(np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))),
                "spectral_rolloff": float(np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr))),
                "zero_crossing_rate": float(np.mean(librosa.feature.zero_crossing_rate(y))),
                "rms_energy": float(np.mean(librosa.feature.rms(y=y)))
            }
            
            # MFCC features
            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
            features["mfcc_mean"] = mfcc.mean(axis=1).tolist()
            features["mfcc_std"] = mfcc.std(axis=1).tolist()
            
            return features
            
        except Exception as e:
            logger.error(f"Erro na extra√ß√£o de features: {e}")
            return {}

# Inst√¢ncia global
speech_processor = SpeechProcessor()