"""
ü§ñ PyLab - Image Generator
Gera√ß√£o de imagens usando Stable Diffusion XL
"""

import torch
import logging
import asyncio
import time
from typing import Optional, Dict, Any, List
from PIL import Image
import io
import gc

from ..api.schemas import ImageGenerationRequest, ImageStyle

logger = logging.getLogger("PyLab.ImageGenerator")

class ImageGenerator:
    """Gerador de imagens usando Stable Diffusion XL"""
    
    def __init__(self):
        self.model = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model_loaded = False
        self.model_name = "stabilityai/stable-diffusion-xl-base-1.0"
        
        logger.info(f"Image Generator inicializado - Device: {self.device}")
        
        # N√£o carregar modelo imediatamente (lazy loading)
        # self._load_model()
    
    async def generate(self, request: ImageGenerationRequest) -> bytes:
        """
        Gerar imagem baseada no request
        
        Args:
            request: Par√¢metros de gera√ß√£o
            
        Returns:
            Dados da imagem em bytes
        """
        try:
            start_time = time.time()
            
            logger.info(f"Gerando imagem: {request.prompt[:50]}...")
            logger.info(f"Estilo: {request.style}, Resolu√ß√£o: {request.width}x{request.height}")
            
            # Carregar modelo se necess√°rio
            if not self.model_loaded:
                await self._load_model()
            
            # Preparar prompt baseado no estilo
            enhanced_prompt = self._enhance_prompt(request.prompt, request.style)
            
            # Configurar par√¢metros
            generation_params = {
                "prompt": enhanced_prompt,
                "negative_prompt": request.negative_prompt or self._get_default_negative_prompt(),
                "width": request.width,
                "height": request.height,
                "num_inference_steps": request.steps,
                "guidance_scale": request.guidance_scale,
                "num_images_per_prompt": request.batch_size,
            }
            
            # Adicionar seed se fornecida
            if request.seed is not None:
                generator = torch.Generator(device=self.device).manual_seed(request.seed)
                generation_params["generator"] = generator
            
            # Executar gera√ß√£o real com SDXL
            result = await self._run_inference(generation_params)
            
            generation_time = time.time() - start_time
            logger.info(f"Imagem gerada em {generation_time:.2f}s")
            
            return result
            
        except Exception as e:
            logger.error(f"Erro na gera√ß√£o de imagem: {e}")
            raise
    
    async def _load_model(self):
        """Carregar modelo Stable Diffusion XL"""
        try:
            logger.info("üîÑ Carregando Stable Diffusion XL...")
            
            # Importar bibliotecas necess√°rias
            from diffusers import StableDiffusionXLPipeline, DPMSolverMultistepScheduler
            
            # Carregar modelo SDXL
            logger.info("üì• Baixando/Carregando Stable Diffusion XL...")
            self.model = StableDiffusionXLPipeline.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                use_safetensors=True,
                variant="fp16" if self.device == "cuda" else None,
                cache_dir="/app/models"
            )
            
            # Usar scheduler otimizado para melhor qualidade
            logger.info("‚öôÔ∏è Configurando scheduler otimizado...")
            self.model.scheduler = DPMSolverMultistepScheduler.from_config(
                self.model.scheduler.config,
                use_karras_sigmas=True,
                algorithm_type="dpmsolver++"
            )
            
            # Otimiza√ß√µes para GPU
            if self.device == "cuda":
                logger.info("üöÄ Aplicando otimiza√ß√µes GPU...")
                self.model = self.model.to(self.device)
                
                # Otimiza√ß√µes de mem√≥ria
                self.model.enable_attention_slicing()
                self.model.enable_xformers_memory_efficient_attention()
                
                # Para GPUs com menos VRAM, descomente:
                # self.model.enable_sequential_cpu_offload()
                # self.model.enable_model_cpu_offload()
                
                # Compila√ß√£o para melhor performance (PyTorch 2.0+)
                try:
                    logger.info("‚ö° Compilando modelo para melhor performance...")
                    self.model.unet = torch.compile(self.model.unet, mode="reduce-overhead")
                except Exception as compile_error:
                    logger.warning(f"‚ö†Ô∏è Compila√ß√£o falhou (n√£o cr√≠tico): {compile_error}")
            
            # Verificar se modelo foi carregado corretamente
            if hasattr(self.model, 'unet') and self.model.unet is not None:
                self.model_loaded = True
                logger.info("‚úÖ Stable Diffusion XL carregado e otimizado com sucesso!")
                
                # Log de informa√ß√µes do modelo
                if self.device == "cuda":
                    memory_allocated = torch.cuda.memory_allocated() / 1024**3
                    logger.info(f"üìä Mem√≥ria GPU alocada: {memory_allocated:.2f}GB")
            else:
                raise Exception("Modelo n√£o foi carregado corretamente")
            
        except ImportError as e:
            logger.error(f"‚ùå Erro de importa√ß√£o - bibliotecas n√£o instaladas: {e}")
            logger.error("üí° Execute: pip install diffusers transformers accelerate")
            raise
        except Exception as e:
            logger.error(f"‚ùå Erro ao carregar modelo: {e}")
            logger.error("üí° Verifique se h√° espa√ßo suficiente e conectividade com internet")
            raise
    
    def _enhance_prompt(self, prompt: str, style: ImageStyle) -> str:
        """Melhorar prompt baseado no estilo"""
        style_enhancements = {
            ImageStyle.REALISTIC: "photorealistic, high quality, detailed, 8k resolution",
            ImageStyle.ARTISTIC: "artistic, creative, expressive, masterpiece",
            ImageStyle.ANIME: "anime style, manga style, cel shading, vibrant colors",
            ImageStyle.CONCEPT_ART: "concept art, digital painting, matte painting, dramatic lighting",
            ImageStyle.PHOTOGRAPHY: "professional photography, DSLR, perfect lighting, sharp focus"
        }
        
        enhancement = style_enhancements.get(style, "high quality, detailed")
        return f"{prompt}, {enhancement}"
    
    def _get_default_negative_prompt(self) -> str:
        """Prompt negativo padr√£o para melhor qualidade"""
        return (
            "low quality, blurry, pixelated, distorted, deformed, "
            "ugly, bad anatomy, extra limbs, watermark, signature, "
            "text, letters, words, bad art, amateur"
        )
    
    async def _run_inference(self, params: Dict[str, Any]) -> bytes:
        """
        Executar infer√™ncia do modelo Stable Diffusion XL
        """
        try:
            logger.info("üé® Iniciando gera√ß√£o de imagem com SDXL...")
            start_time = time.time()
            
            # Executar infer√™ncia em thread separada para n√£o bloquear
            def run_generation():
                with torch.inference_mode():
                    # Limpar cache GPU antes da gera√ß√£o
                    if self.device == "cuda":
                        torch.cuda.empty_cache()
                    
                    # Gerar imagem
                    result = self.model(**params)
                    return result.images[0]
            
            # Executar em thread pool para n√£o bloquear event loop
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(run_generation)
                image = future.result(timeout=300)  # 5 minutos timeout
            
            # Converter para bytes
            img_byte_arr = io.BytesIO()
            
            # Salvar com qualidade otimizada
            image.save(
                img_byte_arr, 
                format='PNG', 
                optimize=True,
                compress_level=6  # Balanceio entre qualidade e tamanho
            )
            
            generation_time = time.time() - start_time
            logger.info(f"‚úÖ Imagem gerada em {generation_time:.2f}s")
            
            # Log de estat√≠sticas
            image_size = len(img_byte_arr.getvalue())
            logger.info(f"üìä Tamanho da imagem: {image_size / 1024 / 1024:.2f}MB")
            logger.info(f"üìê Dimens√µes: {image.size}")
            
            return img_byte_arr.getvalue()
            
        except concurrent.futures.TimeoutError:
            logger.error("‚ùå Timeout na gera√ß√£o de imagem (5 minutos)")
            raise Exception("Gera√ß√£o de imagem demorou muito tempo")
        except torch.cuda.OutOfMemoryError:
            logger.error("‚ùå Mem√≥ria GPU insuficiente")
            # Limpar cache e tentar novamente com configura√ß√µes reduzidas
            if self.device == "cuda":
                torch.cuda.empty_cache()
                gc.collect()
            raise Exception("Mem√≥ria GPU insuficiente. Tente reduzir resolu√ß√£o ou batch_size")
        except Exception as e:
            logger.error(f"‚ùå Erro na infer√™ncia: {e}")
            raise
    
    async def _create_placeholder_image(self, request: ImageGenerationRequest) -> bytes:
        """Criar imagem placeholder para desenvolvimento"""
        try:
            # Criar imagem colorida baseada no prompt
            img = Image.new('RGB', (request.width, request.height), color='lightblue')
            
            # Adicionar texto do prompt (simplificado)
            from PIL import ImageDraw, ImageFont
            draw = ImageDraw.Draw(img)
            
            # Usar fonte padr√£o
            try:
                font = ImageFont.load_default()
            except:
                font = None
            
            # Adicionar informa√ß√µes
            text_lines = [
                f"PyLab AI Generated",
                f"Style: {request.style.value}",
                f"Size: {request.width}x{request.height}",
                f"Prompt: {request.prompt[:50]}..."
            ]
            
            y_offset = 50
            for line in text_lines:
                draw.text((50, y_offset), line, fill='darkblue', font=font)
                y_offset += 30
            
            # Converter para bytes
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG', optimize=True)
            return img_byte_arr.getvalue()
            
        except Exception as e:
            logger.error(f"Erro ao criar placeholder: {e}")
            # Fallback: imagem m√≠nima
            img = Image.new('RGB', (512, 512), color='gray')
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG')
            return img_byte_arr.getvalue()
    
    async def _create_placeholder_image_from_params(self, params: Dict[str, Any]) -> bytes:
        """Criar placeholder baseado nos par√¢metros de gera√ß√£o"""
        width = params.get('width', 1024)
        height = params.get('height', 1024)
        prompt = params.get('prompt', 'AI Generated Image')
        
        img = Image.new('RGB', (width, height), color='lightgreen')
        
        from PIL import ImageDraw
        draw = ImageDraw.Draw(img)
        
        # Adicionar texto centralizado
        text = f"Generated: {prompt[:30]}..."
        bbox = draw.textbbox((0, 0), text)
        text_width = bbox[2] - bbox[0]
        text_height = bbox[3] - bbox[1]
        
        x = (width - text_width) // 2
        y = (height - text_height) // 2
        
        draw.text((x, y), text, fill='darkgreen')
        
        # Converter para bytes
        img_byte_arr = io.BytesIO()
        img.save(img_byte_arr, format='PNG', optimize=True)
        return img_byte_arr.getvalue()
    
    def get_model_info(self) -> Dict[str, Any]:
        """Obter informa√ß√µes do modelo"""
        return {
            "name": "Stable Diffusion XL",
            "version": "1.0",
            "device": self.device,
            "loaded": self.model_loaded,
            "memory_usage": self._get_memory_usage(),
            "supported_styles": [style.value for style in ImageStyle],
            "max_resolution": "2048x2048",
            "recommended_steps": "20-50"
        }
    
    def _get_memory_usage(self) -> str:
        """Obter uso de mem√≥ria GPU"""
        if self.device == "cuda" and torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3  # GB
            cached = torch.cuda.memory_reserved() / 1024**3  # GB
            return f"Allocated: {allocated:.1f}GB, Cached: {cached:.1f}GB"
        return "CPU mode - N/A"
    
    def cleanup(self):
        """Limpar recursos"""
        if self.model is not None:
            del self.model
            self.model = None
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        gc.collect()
        logger.info("Image Generator resources cleaned up")

# === UTILITY FUNCTIONS ===

async def test_image_generation():
    """Fun√ß√£o de teste para desenvolvimento"""
    generator = ImageGenerator()
    
    request = ImageGenerationRequest(
        prompt="A beautiful sunset over the ocean",
        style=ImageStyle.REALISTIC,
        width=1024,
        height=1024
    )
    
    try:
        result = await generator.generate(request)
        logger.info(f"Teste conclu√≠do: {len(result)} bytes gerados")
        return result
    finally:
        generator.cleanup()

if __name__ == "__main__":
    # Teste r√°pido
    asyncio.run(test_image_generation())