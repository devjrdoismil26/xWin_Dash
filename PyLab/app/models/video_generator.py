"""
ü§ñ PyLab - Video Generator
Gera√ß√£o de v√≠deos usando ModelScope Text-to-Video
"""

import torch
import logging
import asyncio
import time
import numpy as np
from typing import Optional, Dict, Any, List
from PIL import Image
import io
import gc
import tempfile
import os

from ..api.schemas import VideoGenerationRequest, VideoQuality

logger = logging.getLogger("PyLab.VideoGenerator")

class VideoGenerator:
    """Gerador de v√≠deos usando ModelScope Text-to-Video"""
    
    def __init__(self):
        self.model = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model_loaded = False
        self.model_name = "damo-vilab/text-to-video-ms-1.7b"
        
        logger.info(f"Video Generator inicializado - Device: {self.device}")
        
        # N√£o carregar modelo imediatamente (lazy loading)
        # self._load_model()
    
    async def generate(self, request: VideoGenerationRequest) -> bytes:
        """
        Gerar v√≠deo baseado no request
        
        Args:
            request: Par√¢metros de gera√ß√£o
            
        Returns:
            Dados do v√≠deo em bytes
        """
        try:
            start_time = time.time()
            
            logger.info(f"Gerando v√≠deo: {request.prompt[:50]}...")
            logger.info(f"Dura√ß√£o: {request.duration}s, Qualidade: {request.quality}, FPS: {request.fps}")
            
            # Carregar modelo se necess√°rio
            if not self.model_loaded:
                await self._load_model()
            
            # Preparar par√¢metros de gera√ß√£o
            generation_params = {
                "prompt": request.prompt,
                "negative_prompt": request.negative_prompt or self._get_default_negative_prompt(),
                "num_frames": self._calculate_frames(request.duration, request.fps),
                "fps": request.fps,
                "quality": request.quality,
                "width": self._get_resolution_width(request.quality),
                "height": self._get_resolution_height(request.quality),
            }
            
            # Adicionar seed se fornecida
            if request.seed is not None:
                generation_params["seed"] = request.seed
            
            # Executar gera√ß√£o real com ModelScope T2V
            result = await self._run_inference(generation_params)
            
            generation_time = time.time() - start_time
            logger.info(f"V√≠deo gerado em {generation_time:.2f}s")
            
            return result
            
        except Exception as e:
            logger.error(f"Erro na gera√ß√£o de v√≠deo: {e}")
            raise
    
    async def _load_model(self):
        """Carregar modelo ModelScope Text-to-Video"""
        try:
            logger.info("üîÑ Carregando ModelScope Text-to-Video...")
            
            # Importar bibliotecas necess√°rias
            from diffusers import DiffusionPipeline
            import torch
            
            # Carregar modelo ModelScope T2V
            logger.info("üì• Baixando/Carregando ModelScope Text-to-Video...")
            self.model = DiffusionPipeline.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                variant="fp16" if self.device == "cuda" else None,
                cache_dir="/app/models",
                custom_pipeline="text_to_video_ms_1_7b"
            )
            
            # Otimiza√ß√µes para GPU
            if self.device == "cuda":
                logger.info("üöÄ Aplicando otimiza√ß√µes GPU para v√≠deo...")
                self.model = self.model.to(self.device)
                
                # Otimiza√ß√µes de mem√≥ria (v√≠deo consome mais VRAM)
                self.model.enable_attention_slicing()
                self.model.enable_vae_slicing()
                
                # Verificar VRAM dispon√≠vel
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
                logger.info(f"üìä VRAM dispon√≠vel: {gpu_memory:.1f}GB")
                
                # Para GPUs com menos VRAM, usar CPU offload
                if gpu_memory < 12:  # < 12GB
                    logger.info("‚ö†Ô∏è VRAM baixa, habilitando CPU offload...")
                    self.model.enable_sequential_cpu_offload()
                    self.model.enable_model_cpu_offload()
                
                # Compila√ß√£o para melhor performance (se dispon√≠vel)
                try:
                    logger.info("‚ö° Compilando modelo de v√≠deo...")
                    self.model.unet = torch.compile(self.model.unet, mode="reduce-overhead")
                except Exception as compile_error:
                    logger.warning(f"‚ö†Ô∏è Compila√ß√£o falhou (n√£o cr√≠tico): {compile_error}")
            
            # Verificar se modelo foi carregado corretamente
            if hasattr(self.model, 'unet') and self.model.unet is not None:
                self.model_loaded = True
                logger.info("‚úÖ ModelScope Text-to-Video carregado e otimizado!")
                
                # Log de informa√ß√µes do modelo
                if self.device == "cuda":
                    memory_allocated = torch.cuda.memory_allocated() / 1024**3
                    logger.info(f"üìä Mem√≥ria GPU alocada: {memory_allocated:.2f}GB")
            else:
                raise Exception("Modelo de v√≠deo n√£o foi carregado corretamente")
            
        except ImportError as e:
            logger.error(f"‚ùå Erro de importa√ß√£o - bibliotecas n√£o instaladas: {e}")
            logger.error("üí° Execute: pip install diffusers transformers accelerate")
            raise
        except Exception as e:
            logger.error(f"‚ùå Erro ao carregar modelo de v√≠deo: {e}")
            logger.error("üí° Verifique conectividade e espa√ßo em disco")
            raise
    
    def _calculate_frames(self, duration: int, fps: int) -> int:
        """Calcular n√∫mero de frames baseado na dura√ß√£o e FPS"""
        return duration * fps
    
    def _get_resolution_width(self, quality: VideoQuality) -> int:
        """Obter largura baseada na qualidade"""
        resolutions = {
            VideoQuality.HD: 1280,
            VideoQuality.FULL_HD: 1920,
            VideoQuality.FOUR_K: 3840
        }
        return resolutions.get(quality, 1280)
    
    def _get_resolution_height(self, quality: VideoQuality) -> int:
        """Obter altura baseada na qualidade"""
        resolutions = {
            VideoQuality.HD: 720,
            VideoQuality.FULL_HD: 1080,
            VideoQuality.FOUR_K: 2160
        }
        return resolutions.get(quality, 720)
    
    def _get_default_negative_prompt(self) -> str:
        """Prompt negativo padr√£o para v√≠deos"""
        return (
            "low quality, blurry, pixelated, distorted, static, "
            "flickering, artifacts, noise, watermark, text, "
            "deformed objects, unnatural motion"
        )
    
    async def _run_inference(self, params: Dict[str, Any]) -> bytes:
        """
        Executar infer√™ncia do modelo ModelScope Text-to-Video
        """
        try:
            logger.info("üé¨ Iniciando gera√ß√£o de v√≠deo com ModelScope T2V...")
            start_time = time.time()
            
            # Executar infer√™ncia em thread separada para n√£o bloquear
            def run_generation():
                with torch.inference_mode():
                    # Limpar cache GPU antes da gera√ß√£o
                    if self.device == "cuda":
                        torch.cuda.empty_cache()
                    
                    # Gerar v√≠deo usando ModelScope
                    result = self.model(
                        params["prompt"],
                        negative_prompt=params["negative_prompt"],
                        num_frames=min(params["num_frames"], 16),  # Limitar frames para estabilidade
                        height=params["height"],
                        width=params["width"],
                        num_inference_steps=25,  # Balanceio qualidade/velocidade
                        guidance_scale=9.0,      # Otimizado para ModelScope
                    )
                    return result.frames[0]  # Primeira sequ√™ncia
            
            # Executar em thread pool para n√£o bloquear event loop
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                future = executor.submit(run_generation)
                video_frames = future.result(timeout=600)  # 10 minutos timeout
            
            # Converter frames para v√≠deo
            logger.info("üîÑ Convertendo frames para v√≠deo...")
            video_bytes = await self._frames_to_video_real(video_frames, params["fps"])
            
            generation_time = time.time() - start_time
            logger.info(f"‚úÖ V√≠deo gerado em {generation_time:.2f}s")
            
            # Log de estat√≠sticas
            video_size = len(video_bytes)
            logger.info(f"üìä Tamanho do v√≠deo: {video_size / 1024 / 1024:.2f}MB")
            logger.info(f"üìê Frames gerados: {len(video_frames)}")
            
            return video_bytes
            
        except concurrent.futures.TimeoutError:
            logger.error("‚ùå Timeout na gera√ß√£o de v√≠deo (10 minutos)")
            raise Exception("Gera√ß√£o de v√≠deo demorou muito tempo")
        except torch.cuda.OutOfMemoryError:
            logger.error("‚ùå Mem√≥ria GPU insuficiente para v√≠deo")
            # Limpar cache e sugerir redu√ß√£o de par√¢metros
            if self.device == "cuda":
                torch.cuda.empty_cache()
                gc.collect()
            raise Exception("Mem√≥ria GPU insuficiente. Tente reduzir resolu√ß√£o, dura√ß√£o ou FPS")
        except Exception as e:
            logger.error(f"‚ùå Erro na infer√™ncia de v√≠deo: {e}")
            raise
    
    async def _create_placeholder_video(self, request: VideoGenerationRequest) -> bytes:
        """Criar v√≠deo placeholder para desenvolvimento"""
        try:
            logger.info("Criando v√≠deo placeholder...")
            
            # Criar algumas imagens para simular frames
            width = self._get_resolution_width(request.quality)
            height = self._get_resolution_height(request.quality)
            num_frames = self._calculate_frames(request.duration, request.fps)
            
            frames = []
            
            for i in range(min(num_frames, 30)):  # Limitar a 30 frames para placeholder
                # Criar frame com cor gradiente
                color_value = int(255 * (i / 30))
                color = (color_value, 100, 255 - color_value)
                
                img = Image.new('RGB', (width, height), color=color)
                
                # Adicionar texto do frame
                from PIL import ImageDraw, ImageFont
                draw = ImageDraw.Draw(img)
                
                try:
                    font = ImageFont.load_default()
                except:
                    font = None
                
                # Informa√ß√µes do frame
                text_lines = [
                    f"PyLab Video - Frame {i+1}/{num_frames}",
                    f"Prompt: {request.prompt[:40]}...",
                    f"Quality: {request.quality.value}",
                    f"Duration: {request.duration}s @ {request.fps}fps"
                ]
                
                y_offset = 50
                for line in text_lines:
                    draw.text((50, y_offset), line, fill='white', font=font)
                    y_offset += 40
                
                frames.append(img)
            
            # Converter frames para v√≠deo usando moviepy
            video_bytes = await self._frames_to_video_bytes(frames, request.fps)
            
            return video_bytes
            
        except Exception as e:
            logger.error(f"Erro ao criar placeholder de v√≠deo: {e}")
            # Fallback: criar arquivo v√≠deo minimal
            return await self._create_minimal_video()
    
    async def _create_placeholder_video_from_params(self, params: Dict[str, Any]) -> bytes:
        """Criar placeholder baseado nos par√¢metros"""
        width = params.get('width', 1280)
        height = params.get('height', 720)
        num_frames = params.get('num_frames', 60)
        fps = params.get('fps', 24)
        prompt = params.get('prompt', 'AI Generated Video')
        
        frames = []
        
        for i in range(min(num_frames, 24)):  # 1 segundo de v√≠deo
            # Criar frame animado
            color_r = int(128 + 127 * np.sin(i * 0.3))
            color_g = int(128 + 127 * np.cos(i * 0.2))
            color_b = int(128 + 127 * np.sin(i * 0.4))
            
            img = Image.new('RGB', (width, height), color=(color_r, color_g, color_b))
            
            from PIL import ImageDraw
            draw = ImageDraw.Draw(img)
            
            # Texto animado
            text = f"Generated: {prompt[:20]}... Frame {i+1}"
            text_x = 50 + int(20 * np.sin(i * 0.5))
            text_y = height // 2
            
            draw.text((text_x, text_y), text, fill='white')
            
            frames.append(img)
        
        return await self._frames_to_video_bytes(frames, fps)
    
    async def _frames_to_video_real(self, video_frames, fps: int) -> bytes:
        """Converter frames do modelo para bytes de v√≠deo otimizado"""
        try:
            import tempfile
            import os
            import numpy as np
            from moviepy.editor import ImageSequenceClip
            
            with tempfile.TemporaryDirectory() as temp_dir:
                # Converter frames tensor para PIL Images
                pil_frames = []
                for i, frame in enumerate(video_frames):
                    # Converter de tensor para numpy array
                    if isinstance(frame, torch.Tensor):
                        frame_np = frame.cpu().numpy()
                        # Normalizar de [-1, 1] para [0, 255]
                        frame_np = ((frame_np + 1.0) * 127.5).astype(np.uint8)
                    else:
                        frame_np = np.array(frame)
                    
                    # Converter para PIL
                    if frame_np.shape[0] == 3:  # CHW format
                        frame_np = np.transpose(frame_np, (1, 2, 0))  # HWC format
                    
                    pil_frame = Image.fromarray(frame_np)
                    pil_frames.append(pil_frame)
                
                # Salvar frames como arquivos tempor√°rios
                frame_paths = []
                for i, frame in enumerate(pil_frames):
                    frame_path = os.path.join(temp_dir, f"frame_{i:04d}.png")
                    frame.save(frame_path, optimize=True)
                    frame_paths.append(frame_path)
                
                # Criar v√≠deo com moviepy
                clip = ImageSequenceClip(frame_paths, fps=fps)
                
                # Salvar como MP4 otimizado
                video_path = os.path.join(temp_dir, "output.mp4")
                clip.write_videofile(
                    video_path,
                    codec='libx264',
                    audio=False,
                    bitrate="8000k",  # Alta qualidade
                    verbose=False,
                    logger=None,
                    preset='medium'   # Balanceio velocidade/compress√£o
                )
                
                # Ler como bytes
                with open(video_path, 'rb') as f:
                    video_bytes = f.read()
                
                clip.close()
                logger.info(f"‚úÖ V√≠deo convertido: {len(video_bytes) / 1024 / 1024:.2f}MB")
                return video_bytes
                
        except Exception as e:
            logger.error(f"‚ùå Erro na convers√£o de frames reais: {e}")
            # Fallback para placeholder se convers√£o falhar
            logger.warning("Usando fallback para placeholder...")
            return await self._create_minimal_video()
    
    async def _frames_to_video_bytes(self, frames: List[Image.Image], fps: int) -> bytes:
        """Converter lista de frames PIL para bytes de v√≠deo"""
        try:
            # Usar moviepy para criar v√≠deo
            import tempfile
            import os
            
            with tempfile.TemporaryDirectory() as temp_dir:
                # Salvar frames como imagens tempor√°rias
                frame_paths = []
                for i, frame in enumerate(frames):
                    frame_path = os.path.join(temp_dir, f"frame_{i:04d}.png")
                    frame.save(frame_path)
                    frame_paths.append(frame_path)
                
                # Criar v√≠deo usando moviepy
                try:
                    from moviepy.editor import ImageSequenceClip
                    
                    clip = ImageSequenceClip(frame_paths, fps=fps)
                    
                    # Salvar como arquivo tempor√°rio
                    video_path = os.path.join(temp_dir, "output.mp4")
                    clip.write_videofile(
                        video_path,
                        codec='libx264',
                        audio=False,
                        verbose=False,
                        logger=None
                    )
                    
                    # Ler como bytes
                    with open(video_path, 'rb') as f:
                        video_bytes = f.read()
                    
                    clip.close()
                    return video_bytes
                    
                except ImportError:
                    logger.warning("MoviePy n√£o dispon√≠vel, criando v√≠deo m√≠nimo")
                    return await self._create_minimal_video()
                
        except Exception as e:
            logger.error(f"Erro ao converter frames para v√≠deo: {e}")
            return await self._create_minimal_video()
    
    async def _create_minimal_video(self) -> bytes:
        """Criar v√≠deo m√≠nimo como fallback"""
        # Criar um "v√≠deo" que √© apenas dados bytes placeholder
        # Em produ√ß√£o real, isso seria um arquivo MP4 v√°lido m√≠nimo
        placeholder_data = b'\x00\x00\x00\x20ftypmp41\x00\x00\x00\x00mp41isom'  # Header MP4 b√°sico
        placeholder_data += b'\x00' * 1024  # 1KB de dados placeholder
        
        logger.info("Criado v√≠deo m√≠nimo placeholder")
        return placeholder_data
    
    def get_model_info(self) -> Dict[str, Any]:
        """Obter informa√ß√µes do modelo"""
        return {
            "name": "ModelScope Text-to-Video",
            "version": "1.7b",
            "device": self.device,
            "loaded": self.model_loaded,
            "memory_usage": self._get_memory_usage(),
            "supported_qualities": [quality.value for quality in VideoQuality],
            "max_duration": "30s",
            "recommended_fps": "24-30"
        }
    
    def _get_memory_usage(self) -> str:
        """Obter uso de mem√≥ria GPU"""
        if self.device == "cuda" and torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3  # GB
            cached = torch.cuda.memory_reserved() / 1024**3  # GB
            return f"Allocated: {allocated:.1f}GB, Cached: {cached:.1f}GB"
        return "CPU mode - N/A"
    
    def cleanup(self):
        """Limpar recursos"""
        if self.model is not None:
            del self.model
            self.model = None
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        gc.collect()
        logger.info("Video Generator resources cleaned up")

# === UTILITY FUNCTIONS ===

async def test_video_generation():
    """Fun√ß√£o de teste para desenvolvimento"""
    generator = VideoGenerator()
    
    request = VideoGenerationRequest(
        prompt="A cat walking in a garden",
        duration=5,
        quality=VideoQuality.HD,
        fps=24
    )
    
    try:
        result = await generator.generate(request)
        logger.info(f"Teste de v√≠deo conclu√≠do: {len(result)} bytes gerados")
        return result
    finally:
        generator.cleanup()

if __name__ == "__main__":
    # Teste r√°pido
    asyncio.run(test_video_generation())