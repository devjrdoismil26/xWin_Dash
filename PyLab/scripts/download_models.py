#!/usr/bin/env python3
"""
ü§ñ PyLab - Script de Download de Modelos IA
Baixa e configura os modelos necess√°rios para o laborat√≥rio de IA
"""

import os
import sys
import torch
import logging
from pathlib import Path

# Configurar logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger("ModelDownloader")

def check_system():
    """Verificar sistema e requisitos"""
    logger.info("üîç Verificando sistema...")
    
    # Verificar CUDA
    cuda_available = torch.cuda.is_available()
    logger.info(f"CUDA dispon√≠vel: {cuda_available}")
    
    if cuda_available:
        gpu_count = torch.cuda.device_count()
        for i in range(gpu_count):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            logger.info(f"GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
    
    # Verificar espa√ßo em disco
    models_dir = Path("/app/models")
    models_dir.mkdir(parents=True, exist_ok=True)
    
    # Estimar espa√ßo necess√°rio
    estimated_space = 12  # GB (SDXL ~6GB + ModelScope ~6GB)
    logger.info(f"üíæ Espa√ßo estimado necess√°rio: {estimated_space}GB")
    
    return cuda_available

def download_stable_diffusion_xl():
    """Baixar Stable Diffusion XL"""
    try:
        logger.info("üì• Baixando Stable Diffusion XL...")
        
        from diffusers import StableDiffusionXLPipeline, DPMSolverMultistepScheduler
        
        # Download do modelo principal
        logger.info("üîÑ Baixando modelo SDXL base...")
        pipeline = StableDiffusionXLPipeline.from_pretrained(
            "stabilityai/stable-diffusion-xl-base-1.0",
            torch_dtype=torch.float16,
            use_safetensors=True,
            variant="fp16",
            cache_dir="/app/models"
        )
        logger.info("‚úÖ Stable Diffusion XL base baixado!")
        
        # Download do scheduler otimizado
        logger.info("üîÑ Baixando scheduler DPM++...")
        scheduler = DPMSolverMultistepScheduler.from_pretrained(
            "stabilityai/stable-diffusion-xl-base-1.0",
            subfolder="scheduler",
            cache_dir="/app/models"
        )
        logger.info("‚úÖ Scheduler DPM++ baixado!")
        
        # Opcional: SDXL Refiner para qualidade extra
        logger.info("üîÑ Baixando SDXL Refiner (opcional)...")
        try:
            refiner = StableDiffusionXLPipeline.from_pretrained(
                "stabilityai/stable-diffusion-xl-refiner-1.0",
                torch_dtype=torch.float16,
                use_safetensors=True,
                variant="fp16",
                cache_dir="/app/models"
            )
            logger.info("‚úÖ SDXL Refiner baixado!")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Refiner n√£o baixado (n√£o cr√≠tico): {e}")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao baixar Stable Diffusion XL: {e}")
        return False

def download_modelscope_t2v():
    """Baixar ModelScope Text-to-Video"""
    try:
        logger.info("üì• Baixando ModelScope Text-to-Video...")
        
        from diffusers import DiffusionPipeline
        
        # Download do modelo de v√≠deo
        logger.info("üîÑ Baixando ModelScope T2V 1.7B...")
        video_pipeline = DiffusionPipeline.from_pretrained(
            "damo-vilab/text-to-video-ms-1.7b",
            torch_dtype=torch.float16,
            variant="fp16",
            cache_dir="/app/models",
            custom_pipeline="text_to_video_ms_1_7b"
        )
        logger.info("‚úÖ ModelScope Text-to-Video baixado!")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Erro ao baixar ModelScope T2V: {e}")
        return False

def verify_models():
    """Verificar se modelos foram baixados corretamente"""
    logger.info("üîç Verificando modelos baixados...")
    
    models_dir = Path("/app/models")
    
    # Verificar estrutura de arquivos
    if not models_dir.exists():
        logger.error("‚ùå Diret√≥rio de modelos n√£o existe")
        return False
    
    # Listar conte√∫do
    total_size = 0
    for root, dirs, files in os.walk(models_dir):
        for file in files:
            file_path = Path(root) / file
            try:
                size = file_path.stat().st_size
                total_size += size
            except:
                pass
    
    total_size_gb = total_size / (1024**3)
    logger.info(f"üìä Total de modelos baixados: {total_size_gb:.2f}GB")
    
    # Verificar se h√° arquivos suficientes (estimativa)
    if total_size_gb < 3:  # Pelo menos 3GB
        logger.warning("‚ö†Ô∏è Tamanho dos modelos parece insuficiente")
        return False
    
    logger.info("‚úÖ Verifica√ß√£o de modelos conclu√≠da!")
    return True

def cleanup_cache():
    """Limpar cache desnecess√°rio"""
    logger.info("üßπ Limpando cache...")
    
    try:
        # Limpar cache do Hugging Face
        import shutil
        cache_dirs = [
            Path.home() / ".cache" / "huggingface",
            Path("/root/.cache/huggingface") if Path("/root").exists() else None
        ]
        
        for cache_dir in cache_dirs:
            if cache_dir and cache_dir.exists():
                # Manter apenas os modelos, remover cache tempor√°rio
                temp_dirs = cache_dir.glob("**/tmp*")
                for temp_dir in temp_dirs:
                    if temp_dir.is_dir():
                        shutil.rmtree(temp_dir, ignore_errors=True)
                        logger.info(f"üóëÔ∏è Removido: {temp_dir}")
        
        # Limpar cache PyTorch
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            logger.info("üóëÔ∏è Cache GPU limpo")
        
        logger.info("‚úÖ Limpeza conclu√≠da!")
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Erro na limpeza (n√£o cr√≠tico): {e}")

def main():
    """Fun√ß√£o principal"""
    logger.info("üöÄ Iniciando download de modelos PyLab...")
    
    # Verificar sistema
    cuda_available = check_system()
    
    success_count = 0
    total_models = 2
    
    # Download Stable Diffusion XL
    if download_stable_diffusion_xl():
        success_count += 1
    
    # Download ModelScope T2V
    if download_modelscope_t2v():
        success_count += 1
    
    # Verificar modelos
    models_ok = verify_models()
    
    # Limpar cache
    cleanup_cache()
    
    # Resultado final
    logger.info("=" * 50)
    if success_count == total_models and models_ok:
        logger.info("üéâ TODOS OS MODELOS BAIXADOS COM SUCESSO!")
        logger.info("‚úÖ PyLab est√° pronto para gerar m√≠dia de alta qualidade!")
        return 0
    else:
        logger.error(f"‚ùå Apenas {success_count}/{total_models} modelos baixados")
        logger.error("üí° Verifique conectividade e espa√ßo em disco")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)